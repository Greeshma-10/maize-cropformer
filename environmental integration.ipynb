{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d964dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found the following unique location IDs:\n",
      "- BJ\n",
      "- HeB\n",
      "- HN\n",
      "- JL\n",
      "- LN\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to your phenotype file on Google Drive\n",
    "phenotype_file = 'C:/Users/GREESHMA/Desktop/cropformer/dataset/Ncii_2015_5locs_hybrids.txt'\n",
    "\n",
    "try:\n",
    "    # Read the file\n",
    "    pheno_df = pd.read_csv(phenotype_file, sep='\\s+')\n",
    "    \n",
    "    # Get the unique values from the 'loc' column\n",
    "    unique_locations = pheno_df['loc'].unique()\n",
    "    \n",
    "    print(\"‚úÖ Found the following unique location IDs:\")\n",
    "    for loc_id in unique_locations:\n",
    "        print(f\"- {loc_id}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå Error: Could not find the phenotype file at '{phenotype_file}'. Please check the path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1020c8d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting to geocode locations...\n",
      "Searching for: 'Beijing, China'...\n",
      "  -> Found: Lat 40.1906, Lon 116.4121\n",
      "Searching for: 'Hebei, China'...\n",
      "  -> Found: Lat 39.0000, Lon 116.0000\n",
      "Searching for: 'Henan, China'...\n",
      "  -> Found: Lat 34.0000, Lon 114.0000\n",
      "Searching for: 'Jilin, China'...\n",
      "  -> Found: Lat 43.5826, Lon 126.1266\n",
      "Searching for: 'Liaoning, China'...\n",
      "  -> Found: Lat 41.2374, Lon 122.9955\n",
      "\n",
      "--- Geocoding Complete ---\n",
      "  Location ID        Full Name   Latitude   Longitude\n",
      "0          BJ   Beijing, China  40.190632  116.412144\n",
      "1         HeB     Hebei, China  39.000000  116.000000\n",
      "2          HN     Henan, China  34.000000  114.000000\n",
      "3          JL     Jilin, China  43.582583  126.126618\n",
      "4          LN  Liaoning, China  41.237411  122.995547\n",
      "\n",
      "‚úÖ Results have been saved to 'location/location_coordinates.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from geopy.geocoders import Nominatim\n",
    "import time\n",
    "\n",
    "# --- Configuration ---\n",
    "# A dictionary mapping your location IDs to their full names for searching\n",
    "location_map = {\n",
    "    'BJ': 'Beijing, China',\n",
    "    'HeB': 'Hebei, China',\n",
    "    'HN': 'Henan, China',\n",
    "    'JL': 'Jilin, China',\n",
    "    'LN': 'Liaoning, China'\n",
    "}\n",
    "output_file = 'location/location_coordinates.csv'\n",
    "# ---------------------\n",
    "\n",
    "print(\"üöÄ Starting to geocode locations...\")\n",
    "\n",
    "# Initialize the geocoder (Nominatim uses OpenStreetMap data)\n",
    "# A unique user_agent is required by their terms of service\n",
    "geolocator = Nominatim(user_agent=\"crop-former-project-greeshma\")\n",
    "\n",
    "results_list = []\n",
    "\n",
    "for loc_id, search_query in location_map.items():\n",
    "    print(f\"Searching for: '{search_query}'...\")\n",
    "    try:\n",
    "        location = geolocator.geocode(search_query, timeout=10)\n",
    "        if location:\n",
    "            results_list.append({\n",
    "                'Location ID': loc_id,\n",
    "                'Full Name': search_query,\n",
    "                'Latitude': location.latitude,\n",
    "                'Longitude': location.longitude\n",
    "            })\n",
    "            print(f\"  -> Found: Lat {location.latitude:.4f}, Lon {location.longitude:.4f}\")\n",
    "        else:\n",
    "            results_list.append({\n",
    "                'Location ID': loc_id,\n",
    "                'Full Name': search_query,\n",
    "                'Latitude': 'Not Found',\n",
    "                'Longitude': 'Not Found'\n",
    "            })\n",
    "            print(f\"  -> Could not find coordinates for '{search_query}'.\")\n",
    "        \n",
    "        # Be respectful to the free API and wait a second between requests\n",
    "        time.sleep(1)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while searching for '{search_query}': {e}\")\n",
    "\n",
    "# Convert the results to a pandas DataFrame for a nice display\n",
    "results_df = pd.DataFrame(results_list)\n",
    "\n",
    "print(\"\\n--- Geocoding Complete ---\")\n",
    "print(results_df)\n",
    "\n",
    "# Save the results to a CSV file for later use\n",
    "results_df.to_csv(output_file, index=False)\n",
    "print(f\"\\n‚úÖ Results have been saved to '{output_file}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18da0ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Original Coordinates ---\n",
      "  Location ID        Full Name   Latitude   Longitude\n",
      "0          BJ   Beijing, China  40.190632  116.412144\n",
      "1         HeB     Hebei, China  39.000000  116.000000\n",
      "2          HN     Henan, China  34.000000  114.000000\n",
      "3          JL     Jilin, China  43.728967  126.199737\n",
      "4          LN  Liaoning, China  41.237411  122.995547\n",
      "\n",
      "--- Modified Coordinates ---\n",
      "  Location ID        Full Name   Latitude   Longitude\n",
      "0          BJ   Beijing, China  40.130000  116.660000\n",
      "1         HeB     Hebei, China  39.000000  116.000000\n",
      "2          HN     Henan, China  34.000000  114.000000\n",
      "3          JL     Jilin, China  43.728967  126.199737\n",
      "4          LN  Liaoning, China  41.237411  122.995547\n",
      "\n",
      "‚úÖ Successfully updated 'location/location_coordinates.csv' with the new coordinates for Beijing.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- Configuration ---\n",
    "coordinates_file = 'location/location_coordinates.csv'\n",
    "\n",
    "# Corrected rural coordinates for Beijing (Shunyi District)\n",
    "beijing_new_lat = 40.13\n",
    "beijing_new_lon = 116.66\n",
    "# ---------------------\n",
    "\n",
    "try:\n",
    "    # Read the existing coordinates file\n",
    "    df = pd.read_csv(coordinates_file)\n",
    "    \n",
    "    print(\"--- Original Coordinates ---\")\n",
    "    print(df)\n",
    "    \n",
    "    # Find the row where 'Location ID' is 'BJ' and update its Latitude and Longitude\n",
    "    df.loc[df['Location ID'] == 'BJ', ['Latitude', 'Longitude']] = [beijing_new_lat, beijing_new_lon]\n",
    "    \n",
    "    # Save the modified DataFrame back to the same file, overwriting it\n",
    "    df.to_csv(coordinates_file, index=False)\n",
    "    \n",
    "    print(\"\\n--- Modified Coordinates ---\")\n",
    "    print(df)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Successfully updated '{coordinates_file}' with the new coordinates for Beijing.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå Error: The file '{coordinates_file}' was not found.\")\n",
    "except KeyError:\n",
    "    print(\"‚ùå Error: Could not find the column 'Location ID' in the file. Please check the column name.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9afc69ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.2\n"
     ]
    }
   ],
   "source": [
    "import netCDF4\n",
    "print(netCDF4.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7837f658",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5285de5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecc059e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting to fetch historical weather (Temp & Rain) from NASA POWER...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Fetching locations:   0%|                                                                        | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  -> BJ: Avg Temp = 22.97¬∞C, Total Rain = 429.97 mm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Fetching locations:  20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                   | 1/5 [00:02<00:10,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  -> HeB: Avg Temp = 24.33¬∞C, Total Rain = 418.85 mm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Fetching locations:  40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                      | 2/5 [00:05<00:07,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  -> HN: Avg Temp = 24.89¬∞C, Total Rain = 455.20 mm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Fetching locations:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                         | 3/5 [00:07<00:04,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  -> JL: Avg Temp = 18.45¬∞C, Total Rain = 453.84 mm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Fetching locations:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè            | 4/5 [00:09<00:02,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  -> LN: Avg Temp = 20.52¬∞C, Total Rain = 525.80 mm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching locations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:11<00:00,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Weather Data Fetching Complete ---\n",
      "  Location_ID  Avg_Season_Temp_C  Total_Rain_mm\n",
      "0          BJ              22.97         429.97\n",
      "1         HeB              24.33         418.85\n",
      "2          HN              24.89         455.20\n",
      "3          JL              18.45         453.84\n",
      "4          LN              20.52         525.80\n",
      "\n",
      "‚úÖ Results have been saved to 'location/location_weather.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "coordinates_file = 'location/location_coordinates.csv'\n",
    "output_file = 'location/location_weather.csv'\n",
    "YEAR = 2015\n",
    "START = f\"{YEAR}0501\"  # May 1st, 2015\n",
    "END   = f\"{YEAR}0930\"  # Sep 30th, 2015\n",
    "# ----------------------\n",
    "\n",
    "print(\"üöÄ Starting to fetch historical weather (Temp & Rain) from NASA POWER...\")\n",
    "\n",
    "try:\n",
    "    locations_df = pd.read_csv(coordinates_file)\n",
    "    weather_results = []\n",
    "\n",
    "    for index, row in tqdm(locations_df.iterrows(), total=len(locations_df), desc=\"Fetching locations\"):\n",
    "        loc_id = row['Location ID']\n",
    "        lat = row['Latitude']\n",
    "        lon = row['Longitude']\n",
    "\n",
    "        # --- MODIFIED: Added PRECTOTCORR to parameters ---\n",
    "        url = (\n",
    "            f\"https://power.larc.nasa.gov/api/temporal/daily/point\"\n",
    "            f\"?parameters=T2M,PRECTOTCORR\"  # Requesting Temp and Precipitation\n",
    "            f\"&start={START}&end={END}\"\n",
    "            f\"&latitude={lat}&longitude={lon}\"\n",
    "            f\"&community=AG\"\n",
    "            f\"&format=JSON\"\n",
    "        )\n",
    "\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "\n",
    "            if \"properties\" in data and \"parameter\" in data[\"properties\"]:\n",
    "                params = data[\"properties\"][\"parameter\"]\n",
    "                \n",
    "                # Calculate average temperature\n",
    "                temps = [temp for temp in params.get(\"T2M\", {}).values() if temp > -990]\n",
    "                avg_temp = np.mean(temps) if temps else None\n",
    "\n",
    "                # Calculate total precipitation\n",
    "                rains = [rain for rain in params.get(\"PRECTOTCORR\", {}).values() if rain > -990]\n",
    "                total_rain = np.sum(rains) if rains else None\n",
    "                \n",
    "                weather_results.append({\n",
    "                    \"Location_ID\": loc_id,\n",
    "                    \"Avg_Season_Temp_C\": avg_temp,\n",
    "                    \"Total_Rain_mm\": total_rain\n",
    "                })\n",
    "                print(f\"\\n  -> {loc_id}: Avg Temp = {avg_temp:.2f}¬∞C, Total Rain = {total_rain:.2f} mm\")\n",
    "            else:\n",
    "                print(f\"\\nWarning: No data returned for {loc_id}\")\n",
    "        else:\n",
    "            print(f\"\\nWarning: Failed to fetch data for {loc_id}. Status: {response.status_code}\")\n",
    "\n",
    "        # avoid hitting request limits\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    # Save results\n",
    "    weather_df = pd.DataFrame(weather_results)\n",
    "    if not os.path.exists('location'):\n",
    "        os.makedirs('location')\n",
    "    weather_df.to_csv(output_file, index=False)\n",
    "\n",
    "    print(\"\\n--- Weather Data Fetching Complete ---\")\n",
    "    print(weather_df.round(2))\n",
    "    print(f\"\\n‚úÖ Results have been saved to '{output_file}'\")\n",
    "\n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    print(f\"\\n‚ùå An unexpected error occurred: {e}\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de7ac91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting to fetch soil data from SoilGrids...\n",
      "Reading coordinates from 'location/location_coordinates.csv'...\n",
      "Applying coordinate fix for Beijing (BJ)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching locations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:12<00:00,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Beijing (BJ) data is missing after API call. Using data from nearby Hebei (HeB) as a substitute.\n",
      "\n",
      "--- Soil Data Fetching Complete ---\n",
      "  Location_ID  Soil_pH  Soil_OrgCarbon\n",
      "0          BJ     8.05           177.0\n",
      "1         HeB     8.05           177.0\n",
      "2          HN     7.75           222.0\n",
      "3          JL     6.55           286.5\n",
      "4          LN     6.75           218.0\n",
      "\n",
      "‚úÖ Results have been saved to 'location/location_soil_data.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import traceback\n",
    "\n",
    "# ========================\n",
    "# CONFIGURATION\n",
    "# ========================\n",
    "SOILGRID_URL = \"https://rest.isric.org/soilgrids/v2.0/properties/query\"\n",
    "coordinates_file = 'location/location_coordinates.csv'\n",
    "output_file = 'location/location_soil_data.csv'\n",
    "# ------------------------\n",
    "\n",
    "print(\"üöÄ Starting to fetch soil data from SoilGrids...\")\n",
    "\n",
    "def fetch_soil(lat, lon):\n",
    "    \"\"\"Fetches soil pH and organic carbon from SoilGrids API with robust parsing.\"\"\"\n",
    "    params = {\n",
    "        \"lon\": lon, \"lat\": lat, \"property\": [\"phh2o\", \"ocd\"],\n",
    "        \"depth\": [\"0-5cm\", \"5-15cm\"], \"value\": [\"mean\"]\n",
    "    }\n",
    "    try:\n",
    "        r = requests.get(SOILGRID_URL, params=params, timeout=30)\n",
    "        if r.status_code != 200:\n",
    "            return None, None\n",
    "        \n",
    "        data = r.json()\n",
    "        ph_mean, ocd_mean = None, None\n",
    "        \n",
    "        layers = data.get(\"properties\", {}).get(\"layers\", [])\n",
    "        for layer in layers:\n",
    "            name = layer.get(\"name\")\n",
    "            depth_values = [\n",
    "                d['values']['mean'] for d in layer.get('depths', [])\n",
    "                if 'values' in d and 'mean' in d['values'] and d['values']['mean'] is not None\n",
    "            ]\n",
    "            if not depth_values: continue\n",
    "            avg_value = np.mean(depth_values)\n",
    "            if name == \"phh2o\": ph_mean = avg_value / 10.0\n",
    "            elif name == \"ocd\": ocd_mean = avg_value\n",
    "        \n",
    "        return ph_mean, ocd_mean\n",
    "    except Exception:\n",
    "        return None, None\n",
    "\n",
    "# --- Main Logic ---\n",
    "try:\n",
    "    # 1. Read coordinates from the file\n",
    "    print(f\"Reading coordinates from '{coordinates_file}'...\")\n",
    "    locations_df = pd.read_csv(coordinates_file)\n",
    "    \n",
    "    # 2. Apply the fix for Beijing's coordinates in the DataFrame\n",
    "    print(\"Applying coordinate fix for Beijing (BJ)...\")\n",
    "    locations_df.loc[locations_df['Location ID'] == 'BJ', ['Latitude', 'Longitude']] = [40.13, 116.66] # Shunyi District\n",
    "\n",
    "    # 3. Loop through the DataFrame and fetch data\n",
    "    soil_results = []\n",
    "    for index, row in tqdm(locations_df.iterrows(), total=len(locations_df), desc=\"Fetching locations\"):\n",
    "        loc_id = row['Location ID']\n",
    "        lat = row['Latitude']\n",
    "        lon = row['Longitude']\n",
    "        \n",
    "        soil_ph, soil_oc = fetch_soil(lat, lon)\n",
    "        \n",
    "        soil_results.append({\n",
    "            \"Location_ID\": loc_id,\n",
    "            \"Soil_pH\": round(soil_ph, 2) if soil_ph is not None else np.nan,\n",
    "            \"Soil_OrgCarbon\": round(soil_oc, 2) if soil_oc is not None else np.nan\n",
    "        })\n",
    "        time.sleep(1)\n",
    "\n",
    "    soil_df = pd.DataFrame(soil_results)\n",
    "\n",
    "    # 4. Final safety check: If BJ is still missing, use HeB data\n",
    "    bj_row = soil_df[soil_df['Location_ID'] == 'BJ']\n",
    "    if bj_row.isnull().values.any():\n",
    "        print(\"\\nBeijing (BJ) data is missing after API call. Using data from nearby Hebei (HeB) as a substitute.\")\n",
    "        heb_data = soil_df[soil_df['Location_ID'] == 'HeB'].iloc[0]\n",
    "        soil_df.loc[soil_df['Location_ID'] == 'BJ', ['Soil_pH', 'Soil_OrgCarbon']] = [heb_data['Soil_pH'], heb_data['Soil_OrgCarbon']]\n",
    "\n",
    "    # 5. Save the final, complete DataFrame\n",
    "    if not os.path.exists('location'):\n",
    "        os.makedirs('location')\n",
    "    soil_df.to_csv(output_file, index=False)\n",
    "\n",
    "    print(\"\\n--- Soil Data Fetching Complete ---\")\n",
    "    print(soil_df)\n",
    "    print(f\"\\n‚úÖ Results have been saved to '{output_file}'\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå Error: Could not find the coordinates file at '{coordinates_file}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå An unexpected error occurred: {e}\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9196db71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43a31c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting to merge data using a robust method...\n",
      "Loading data files...\n",
      "Adding environmental data columns...\n",
      "\n",
      "--- Merge Complete ---\n",
      "Final enriched data has 6210 samples and 10004 columns.\n",
      "\n",
      "‚úÖ Successfully saved to 'csv_files/chr10_top10k_snps_with_env.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import traceback\n",
    "\n",
    "# --- Configuration ---\n",
    "geno_file = 'csv_files/chr10_top10k_snps.csv'\n",
    "weather_file = 'location/location_weather.csv'\n",
    "soil_file = 'location/location_soil_data.csv'\n",
    "phenotype_map_file = 'dataset/Ncii_2015_5locs_hybrids.txt'\n",
    "output_file = 'csv_files/chr10_top10k_snps_with_env.csv'\n",
    "# ---------------------\n",
    "\n",
    "print(\"üöÄ Starting to merge data using a robust method...\")\n",
    "\n",
    "try:\n",
    "    # 1. Load all data files\n",
    "    print(\"Loading data files...\")\n",
    "    geno_df = pd.read_csv(geno_file, index_col=0)\n",
    "    weather_df = pd.read_csv(weather_file)\n",
    "    soil_df = pd.read_csv(soil_file)\n",
    "    pheno_map_df = pd.read_csv(phenotype_map_file, sep='\\\\s+')\n",
    "\n",
    "    # 2. Create helper maps (dictionaries) for fast lookups\n",
    "    # Merge weather and soil and set location as the index\n",
    "    env_df = pd.merge(weather_df, soil_df, on='Location_ID').set_index('Location_ID')\n",
    "    temp_map = env_df['Avg_Season_Temp_C'].to_dict()\n",
    "    rain_map = env_df['Total_Rain_mm'].to_dict()  # <-- ADDED\n",
    "    ph_map = env_df['Soil_pH'].to_dict()\n",
    "    oc_map = env_df['Soil_OrgCarbon'].to_dict()\n",
    "    \n",
    "    # Create a map from Sample ID ('Lineid') to Location ID ('loc')\n",
    "    sample_to_loc_map = pheno_map_df.drop_duplicates(subset='Lineid').set_index('Lineid')['loc']\n",
    "    \n",
    "    # 3. Use the .map() method to add new columns without losing the index\n",
    "    print(\"Adding environmental data columns...\")\n",
    "    \n",
    "    # First, find the location for each sample in our main dataframe\n",
    "    loc_col = geno_df.index.map(sample_to_loc_map)\n",
    "    \n",
    "    # Now, use that location to find the correct environmental data from our maps\n",
    "    geno_df['Avg_Temp_C'] = loc_col.map(temp_map)\n",
    "    geno_df['Total_Rain_mm'] = loc_col.map(rain_map) # <-- ADDED\n",
    "    geno_df['Soil_pH'] = loc_col.map(ph_map)\n",
    "    geno_df['Soil_OrgCarbon'] = loc_col.map(oc_map)\n",
    "    \n",
    "    # 4. Save the result. The original index is preserved.\n",
    "    geno_df.to_csv(output_file, index=True)\n",
    "\n",
    "    print(\"\\n--- Merge Complete ---\")\n",
    "    print(f\"Final enriched data has {geno_df.shape[0]} samples and {geno_df.shape[1]} columns.\")\n",
    "    print(f\"\\n‚úÖ Successfully saved to '{output_file}'\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"‚ùå Error: A required file was not found. Please check this path: {e.filename}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå An unexpected error occurred: {e}\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b8f749b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Checking Sample IDs from Genotype File ---\n",
      "Index name: IID\n",
      "First 5 sample IDs:\n",
      "['MG_115_X_MG_1528', 'MG_991_X_MG_1524', 'MG_162_X_MG_1540', 'MG_204_X_MG_1520', 'MG_68_X_MG_1545']\n",
      "\n",
      "==================================================\n",
      "\n",
      "--- Checking Sample IDs from Phenotype File ---\n",
      "Column name: 'Lineid'\n",
      "First 5 unique sample IDs:\n",
      "['MG_298_X_MG_1530', 'MG_298_X_MG_1534', 'MG_298_X_MG_1538', 'MG_298_X_MG_1540', 'MG_298_X_MG_1518']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- Configuration (using the paths from your error message) ---\n",
    "geno_file = 'csv_files/chr10_top10k_snps_with_env.csv'\n",
    "pheno_file = 'dataset/Ncii_2015_5locs_hybrids.txt'\n",
    "# ---------------------\n",
    "\n",
    "try:\n",
    "    print(\"--- Checking Sample IDs from Genotype File ---\")\n",
    "    geno_df = pd.read_csv(geno_file, index_col=0)\n",
    "    print(\"Index name:\", geno_df.index.name)\n",
    "    print(\"First 5 sample IDs:\")\n",
    "    print(geno_df.index[:5].tolist())\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    \n",
    "    print(\"--- Checking Sample IDs from Phenotype File ---\")\n",
    "    pheno_df = pd.read_csv(pheno_file, sep='\\\\s+')\n",
    "    print(\"Column name: 'Lineid'\")\n",
    "    print(\"First 5 unique sample IDs:\")\n",
    "    print(pheno_df['Lineid'].unique()[:5].tolist())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdcb0be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Reading genotype file...\n",
      "\n",
      "--- File Overview ---\n",
      "Shape: (6210, 10005)\n",
      "Columns: ['IID', 'chr10.s_623678', 'chr10.s_623766', 'chr10.s_623848', 'chr10.s_623933', 'chr10.s_623948', 'chr10.s_623982', 'chr10.s_624155', 'chr10.s_624413', 'chr10.s_624647'] ...\n",
      "\n",
      "--- First 5 Rows ---\n",
      "                IID  chr10.s_623678  chr10.s_623766  chr10.s_623848  \\\n",
      "0  MG_115_X_MG_1528               5               3               3   \n",
      "1  MG_991_X_MG_1524               5               3               3   \n",
      "2  MG_162_X_MG_1540               5               3               3   \n",
      "3  MG_204_X_MG_1520               5               3               3   \n",
      "4   MG_68_X_MG_1545               7               9               9   \n",
      "\n",
      "   chr10.s_623933  chr10.s_623948  chr10.s_623982  chr10.s_624155  \\\n",
      "0               3               3               3               6   \n",
      "1               3               3               3               6   \n",
      "2               3               3               3               6   \n",
      "3               3               3               3               6   \n",
      "4               9               0               9               9   \n",
      "\n",
      "   chr10.s_624413  chr10.s_624647  ...  chr10.s_148819830  chr10.s_148820385  \\\n",
      "0               2               5  ...                  4                  0   \n",
      "1               2               5  ...                  5                  3   \n",
      "2               2               5  ...                  4                  0   \n",
      "3               2               5  ...                  5                  3   \n",
      "4               0               7  ...                  4                  0   \n",
      "\n",
      "   chr10.s_148892180  chr10.s_148902701  chr10.s_148951704  chr10.s_148951722  \\\n",
      "0                  7                  0                  9                  0   \n",
      "1                  8                  3                  9                  0   \n",
      "2                  7                  0                  9                  0   \n",
      "3                  8                  3                  9                  0   \n",
      "4                  7                  0                  9                  0   \n",
      "\n",
      "   Avg_Temp_C  Total_Rain_mm  Soil_pH  Soil_OrgCarbon  \n",
      "0   22.968301         429.97     8.05           177.0  \n",
      "1   22.968301         429.97     8.05           177.0  \n",
      "2   22.968301         429.97     8.05           177.0  \n",
      "3   22.968301         429.97     8.05           177.0  \n",
      "4   22.968301         429.97     8.05           177.0  \n",
      "\n",
      "[5 rows x 10005 columns]\n",
      "\n",
      "--- Column name check ---\n",
      "Possible sample ID columns: ['IID']\n",
      "\n",
      "‚úÖ First column 'IID' might contain sample IDs:\n",
      "0    MG_115_X_MG_1528\n",
      "1    MG_991_X_MG_1524\n",
      "2    MG_162_X_MG_1540\n",
      "3    MG_204_X_MG_1520\n",
      "4     MG_68_X_MG_1545\n",
      "Name: IID, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "geno_file = 'csv_files/chr10_top10k_snps_with_env.csv'\n",
    "\n",
    "print(\"üîç Reading genotype file...\")\n",
    "\n",
    "# Try reading first few rows\n",
    "geno_df = pd.read_csv(geno_file)\n",
    "\n",
    "print(\"\\n--- File Overview ---\")\n",
    "print(f\"Shape: {geno_df.shape}\")\n",
    "print(\"Columns:\", geno_df.columns.tolist()[:10], \"...\" if geno_df.shape[1] > 10 else \"\")\n",
    "\n",
    "# Check first few rows to see how the data looks\n",
    "print(\"\\n--- First 5 Rows ---\")\n",
    "print(geno_df.head())\n",
    "\n",
    "# Check if any column name looks like it holds sample identifiers\n",
    "print(\"\\n--- Column name check ---\")\n",
    "possible_id_cols = [col for col in geno_df.columns if 'id' in col.lower() or 'sample' in col.lower() or 'line' in col.lower()]\n",
    "print(\"Possible sample ID columns:\", possible_id_cols)\n",
    "\n",
    "# If first column looks like IDs (non-numeric), show a sample\n",
    "first_col = geno_df.columns[0]\n",
    "if not pd.api.types.is_numeric_dtype(geno_df[first_col]):\n",
    "    print(f\"\\n‚úÖ First column '{first_col}' might contain sample IDs:\")\n",
    "    print(geno_df[first_col].head())\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è First column '{first_col}' seems numeric ‚Äî sample IDs might be missing.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d5d0d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Phase 2: Splitting data into training and testing sets...\n",
      "Loading feature-selected genotype data from 'csv_files/chr10_top10k_snps_with_env.csv'...\n",
      "Loading and averaging phenotype data from 'dataset/Ncii_2015_5locs_hybrids.txt'...\n",
      "Aligning final genotype and phenotype data...\n",
      "Final dataset contains 5831 samples.\n",
      "Splitting data (80% training, 20% testing)...\n",
      "Training set size: 4664 samples\n",
      "Testing set size: 1167 samples\n",
      "Saving output files...\n",
      "‚úÖ Success! Data splitting is complete.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import traceback\n",
    "\n",
    "# --- IMPORTANT: CHOOSE YOUR TRAIT ---\n",
    "# Make sure this is the same trait you used for feature selection.\n",
    "TARGET_TRAIT = 'DTT'\n",
    "# ------------------------------------\n",
    "\n",
    "# --- Configuration ---\n",
    "feature_selected_geno_file = 'csv_files/chr10_top10k_snps_with_env.csv'\n",
    "phenotype_file = 'dataset/Ncii_2015_5locs_hybrids.txt'\n",
    "# ---------------------\n",
    "\n",
    "print(\"üöÄ Starting Phase 2: Splitting data into training and testing sets...\")\n",
    "\n",
    "try:\n",
    "    # 1. Load the data\n",
    "    print(f\"Loading feature-selected genotype data from '{feature_selected_geno_file}'...\")\n",
    "    X = pd.read_csv(feature_selected_geno_file, index_col=0)\n",
    "\n",
    "    print(f\"Loading and averaging phenotype data from '{phenotype_file}'...\")\n",
    "    pheno_df = pd.read_csv(phenotype_file, sep='\\\\s+')\n",
    "    \n",
    "    # --- THIS IS THE NEW, CORRECTED PART ---\n",
    "    # Group by the sample ID and calculate the mean for the target trait\n",
    "    y_series = pheno_df.groupby('Lineid')[TARGET_TRAIT].mean()\n",
    "    # ---------------------------------------\n",
    "\n",
    "    # 2. Align the datasets\n",
    "    print(\"Aligning final genotype and phenotype data...\")\n",
    "    common_samples = X.index.intersection(y_series.index)\n",
    "    X = X.loc[common_samples]\n",
    "    y = y_series.loc[common_samples]\n",
    "    \n",
    "    # Drop any remaining missing values (e.g., if mean results in NaN)\n",
    "    valid_indices = y.dropna().index\n",
    "    y = y.loc[valid_indices]\n",
    "    X = X.loc[valid_indices]\n",
    "\n",
    "    print(f\"Final dataset contains {len(X)} samples.\")\n",
    "\n",
    "    # 3. Split the data (80% train, 20% test)\n",
    "    print(\"Splitting data (80% training, 20% testing)...\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set size: {len(X_train)} samples\")\n",
    "    print(f\"Testing set size: {len(X_test)} samples\")\n",
    "\n",
    "    # 4. Save the four new files\n",
    "    print(\"Saving output files...\")\n",
    "    X_train.to_csv('X_train_environ.csv')\n",
    "    y_train.to_csv('y_train_environ.csv', header=True)\n",
    "    X_test.to_csv('X_test_environ.csv')\n",
    "    y_test.to_csv('y_test_environ.csv', header=True)\n",
    "    \n",
    "    print(\"‚úÖ Success! Data splitting is complete.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå An unexpected error occurred: {e}\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed031579",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cropformer_env)",
   "language": "python",
   "name": "cropformer_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
