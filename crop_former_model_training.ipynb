{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtWDALB26Atb",
        "outputId": "c1d51e80-d55e-4856-f06d-d9955878abac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna lightning"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MysZQaMN7G8n",
        "outputId": "2643f0c7-a1ee-4b1e-ea6b-c1be53079ba5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.5.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting lightning\n",
            "  Downloading lightning-2.5.5-py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (1.16.5)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.43)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.3)\n",
            "Requirement already satisfied: fsspec<2027.0,>=2022.5.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<2027.0,>=2022.5.0->lightning) (2025.3.0)\n",
            "Collecting lightning-utilities<2.0,>=0.10.0 (from lightning)\n",
            "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: torch<4.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from lightning) (2.8.0+cu126)\n",
            "Collecting torchmetrics<3.0,>0.7.0 (from lightning)\n",
            "  Downloading torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: typing-extensions<6.0,>4.5.0 in /usr/local/lib/python3.12/dist-packages (from lightning) (4.15.0)\n",
            "Collecting pytorch-lightning (from lightning)\n",
            "  Downloading pytorch_lightning-2.5.5-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<2027.0,>=2022.5.0->lightning) (3.12.15)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning) (75.2.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.19.1)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.4.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (1.20.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<4.0,>=2.1.0->lightning) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<4.0,>=2.1.0->lightning) (3.0.3)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.12/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (3.10)\n",
            "Downloading optuna-4.5.0-py3-none-any.whl (400 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning-2.5.5-py3-none-any.whl (828 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m828.5/828.5 kB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
            "Downloading torchmetrics-1.8.2-py3-none-any.whl (983 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Downloading pytorch_lightning-2.5.5-py3-none-any.whl (832 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m832.4/832.4 kB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lightning-utilities, colorlog, optuna, torchmetrics, pytorch-lightning, lightning\n",
            "Successfully installed colorlog-6.9.0 lightning-2.5.5 lightning-utilities-0.15.2 optuna-4.5.0 pytorch-lightning-2.5.5 torchmetrics-1.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============\n",
        "# 1. IMPORTS\n",
        "# ==============\n",
        "import optuna\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.optim import Adam\n",
        "from torch.nn import MSELoss\n",
        "from lightning.pytorch import LightningModule\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold\n",
        "import torch.optim as optim\n",
        "import os\n",
        "import functools\n",
        "import torch.nn as nn\n",
        "import time\n",
        "\n",
        "# ========================\n",
        "# 2. MODEL DEFINITION\n",
        "# ========================\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, hidden_size, eps=1e-12):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
        "        self.bias = nn.Parameter(torch.zeros(hidden_size))\n",
        "        self.variance_epsilon = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        u = x.mean(-1, keepdim=True)\n",
        "        s = (x - u).pow(2).mean(-1, keepdim=True)\n",
        "        x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n",
        "        return self.weight * x + self.bias\n",
        "\n",
        "class SelfAttention(LightningModule):\n",
        "    def __init__(self, num_attention_heads, input_size, hidden_size, output_dim=1, kernel_size=3,\n",
        "                 hidden_dropout_prob=0.5, attention_probs_dropout_prob=0.5, learning_rate=0.001):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.num_attention_heads = num_attention_heads\n",
        "        self.attention_head_size = int(hidden_size / num_attention_heads)\n",
        "        self.all_head_size = hidden_size\n",
        "        self.query = torch.nn.Linear(input_size, self.all_head_size)\n",
        "        self.key = torch.nn.Linear(input_size, self.all_head_size)\n",
        "        self.value = torch.nn.Linear(input_size, self.all_head_size)\n",
        "        self.attn_dropout = torch.nn.Dropout(attention_probs_dropout_prob)\n",
        "        self.out_dropout = torch.nn.Dropout(hidden_dropout_prob)\n",
        "        self.dense = torch.nn.Linear(hidden_size, input_size)\n",
        "        self.LayerNorm = torch.nn.LayerNorm(input_size, eps=1e-12)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.out = torch.nn.Linear(input_size, output_dim)\n",
        "        self.cnn = torch.nn.Conv1d(1, 1, kernel_size, stride=1, padding=1)\n",
        "        self.learning_rate = learning_rate\n",
        "        self.loss_fn = MSELoss()\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        cnn_hidden = self.cnn(input_tensor.view(input_tensor.size(0), 1, -1))\n",
        "        # --- FIX #1: This was causing a shape error. It now correctly keeps the 3D shape for the attention mechanism. ---\n",
        "        input_tensor_after_cnn = cnn_hidden\n",
        "\n",
        "        mixed_query_layer = self.query(input_tensor_after_cnn)\n",
        "        mixed_key_layer = self.key(input_tensor_after_cnn)\n",
        "        mixed_value_layer = self.value(input_tensor_after_cnn)\n",
        "        query_layer = mixed_query_layer\n",
        "        key_layer = mixed_key_layer\n",
        "        value_layer = mixed_value_layer\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "        attention_scores = attention_scores / np.sqrt(self.attention_head_size)\n",
        "        attention_probs = torch.nn.Softmax(dim=-1)(attention_scores)\n",
        "        attention_probs = self.attn_dropout(attention_probs)\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "        hidden_states = self.dense(context_layer)\n",
        "        hidden_states = self.out_dropout(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor_after_cnn)\n",
        "        output = self.out(self.relu(hidden_states.view(hidden_states.size(0), -1)))\n",
        "        return output\n",
        "\n",
        "    # (The rest of the class methods are fine)\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_pred = self(x)\n",
        "        loss = self.loss_fn(y_pred, y)\n",
        "        return loss\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_pred = self(x)\n",
        "        val_loss = self.loss_fn(y_pred, y)\n",
        "        return val_loss\n",
        "    def configure_optimizers(self):\n",
        "        return Adam(self.parameters(), lr=self.learning_rate)\n",
        "\n",
        "# ========================\n",
        "# 3. HELPER FUNCTIONS\n",
        "# ========================\n",
        "def objective(trial, x_train, y_train, inner_cv, DEVICE, hidden_dim, output_dim, kernel_size, learning_rate):\n",
        "    # This function is for hyperparameter tuning and seems okay.\n",
        "    num_attention_heads = trial.suggest_categorical('num_attention_heads', [4, 8])\n",
        "    attention_probs_dropout_prob = trial.suggest_categorical('attention_probs_dropout_prob', [0.2, 0.5])\n",
        "    fold_losses = []\n",
        "    for train_idx, valid_idx in inner_cv.split(x_train):\n",
        "        x_inner_train, x_inner_valid = x_train[train_idx], x_train[valid_idx]\n",
        "        y_inner_train, y_inner_valid = y_train[train_idx], y_train[valid_idx]\n",
        "        scaler = StandardScaler()\n",
        "        x_inner_train = scaler.fit_transform(x_inner_train)\n",
        "        x_inner_valid = scaler.transform(x_inner_valid)\n",
        "        x_inner_train_tensor = torch.from_numpy(x_inner_train).float().to(DEVICE)\n",
        "        y_inner_train_tensor = torch.from_numpy(y_inner_train).float().to(DEVICE)\n",
        "        x_inner_valid_tensor = torch.from_numpy(x_inner_valid).float().to(DEVICE)\n",
        "        y_inner_valid_tensor = torch.from_numpy(y_inner_valid).float().to(DEVICE)\n",
        "        train_data = TensorDataset(x_inner_train_tensor, y_inner_train_tensor)\n",
        "        valid_data = TensorDataset(x_inner_valid_tensor, y_inner_valid_tensor)\n",
        "        train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "        valid_loader = DataLoader(valid_data, batch_size=32, shuffle=False)\n",
        "        model = SelfAttention(num_attention_heads, x_inner_train.shape[1], hidden_dim, output_dim,\n",
        "                              hidden_dropout_prob=0.5, kernel_size=kernel_size,\n",
        "                              attention_probs_dropout_prob=attention_probs_dropout_prob).to(DEVICE)\n",
        "        loss_function = torch.nn.MSELoss()\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "        for epoch in range(20):\n",
        "            model.train()\n",
        "            for x_batch, y_batch in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                y_pred = model(x_batch)\n",
        "                loss = loss_function(y_pred, y_batch.reshape(-1, 1))\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "        model.eval()\n",
        "        valid_losses = []\n",
        "        with torch.no_grad():\n",
        "            for x_batch, y_batch in valid_loader:\n",
        "                y_pred = model(x_batch)\n",
        "                loss = loss_function(y_pred, y_batch.reshape(-1, 1))\n",
        "                valid_losses.append(loss.item())\n",
        "        fold_losses.append(np.mean(valid_losses))\n",
        "    return np.mean(fold_losses)\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=10, delta=0):\n",
        "        self.patience = patience\n",
        "        self.delta = delta\n",
        "        self.best_score = -np.Inf\n",
        "        self.counter = 0\n",
        "        self.early_stop = False\n",
        "    def __call__(self, score):\n",
        "        if self.best_score == -np.Inf:\n",
        "            self.best_score = score\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.counter = 0\n",
        "\n",
        "def run_nested_cv_with_early_stopping(data, label, outer_cv, inner_cv, learning_rate, batch_size, hidden_dim,\n",
        "                                      output_dim, kernel_size, patience, DEVICE):\n",
        "    best_corr_coefs = []\n",
        "    time_star = time.time()\n",
        "    for fold, (train_idx, test_idx) in enumerate(outer_cv.split(data)):\n",
        "        model_save_path = f'/content/drive/MyDrive/crop_former_model/best_model_fold_{fold + 1}.pth'\n",
        "        if os.path.exists(model_save_path):\n",
        "            print(f\"✅ Fold {fold + 1} already completed. Model file found. Skipping.\")\n",
        "            continue\n",
        "        print(f\"\\n--- Starting Fold {fold + 1}/5 ---\")\n",
        "        x_train, x_test = data[train_idx], data[test_idx]\n",
        "        y_train, y_test = label[train_idx], label[test_idx]\n",
        "        print(\"Running Optuna for hyperparameter tuning...\")\n",
        "        objective_with_data = functools.partial(objective, x_train=x_train, y_train=y_train, inner_cv=inner_cv,\n",
        "                                                DEVICE=DEVICE, hidden_dim=hidden_dim, output_dim=output_dim,\n",
        "                                                kernel_size=kernel_size, learning_rate=learning_rate)\n",
        "        study = optuna.create_study(direction='minimize')\n",
        "        study.optimize(objective_with_data, n_trials=20)\n",
        "        best_trial = study.best_trial\n",
        "        print(f\"Optuna found best params for Fold {fold + 1}: {best_trial.params}\")\n",
        "        num_attention_heads = best_trial.params['num_attention_heads']\n",
        "        attention_probs_dropout_prob = best_trial.params['attention_probs_dropout_prob']\n",
        "        model = SelfAttention(num_attention_heads, x_train.shape[1], hidden_dim, output_dim,\n",
        "                              hidden_dropout_prob=0.5, kernel_size=kernel_size,\n",
        "                              attention_probs_dropout_prob=attention_probs_dropout_prob).to(DEVICE)\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "        loss_function = torch.nn.MSELoss()\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=10)\n",
        "        scaler = StandardScaler()\n",
        "        x_train = scaler.fit_transform(x_train)\n",
        "        x_test = scaler.transform(x_test)\n",
        "\n",
        "        # --- FIX #2: This was the source of the NameError. It now correctly uses x_train, y_train, etc. ---\n",
        "        x_train_tensor = torch.from_numpy(x_train).float().to(DEVICE)\n",
        "        y_train_tensor = torch.from_numpy(y_train).float().to(DEVICE)\n",
        "        x_test_tensor = torch.from_numpy(x_test).float().to(DEVICE)\n",
        "        y_test_tensor = torch.from_numpy(y_test).float().to(DEVICE)\n",
        "\n",
        "        train_data = TensorDataset(x_train_tensor, y_train_tensor)\n",
        "        test_data = TensorDataset(x_test_tensor, y_test_tensor)\n",
        "        train_loader = DataLoader(train_data, batch_size, shuffle=True)\n",
        "        test_loader = DataLoader(test_data, batch_size, shuffle=False)\n",
        "        early_stopping = EarlyStopping(patience=patience)\n",
        "        best_corr_coef = -float('inf')\n",
        "        print(f\"Starting final training for Fold {fold + 1}...\")\n",
        "        for epoch in range(100):\n",
        "            model.train()\n",
        "            for x_batch, y_batch in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                y_pred = model(x_batch)\n",
        "                loss = loss_function(y_pred, y_batch.reshape(-1, 1))\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "            model.eval()\n",
        "            y_test_preds, y_test_trues = [], []\n",
        "            with torch.no_grad():\n",
        "                for x_batch, y_batch in test_loader:\n",
        "                    y_test_pred = model(x_batch)\n",
        "                    y_test_preds.extend(y_test_pred.cpu().numpy().reshape(-1).tolist())\n",
        "                    y_test_trues.extend(y_batch.cpu().numpy().reshape(-1).tolist())\n",
        "            corr_coef = np.corrcoef(y_test_preds, y_test_trues)[0, 1]\n",
        "            scheduler.step(corr_coef)\n",
        "            if corr_coef > best_corr_coef:\n",
        "                best_corr_coef = corr_coef\n",
        "                torch.save(model.state_dict(), model_save_path)\n",
        "            early_stopping(corr_coef)\n",
        "            if early_stopping.early_stop:\n",
        "                print(f\"Early stopping at epoch {epoch + 1}\")\n",
        "                break\n",
        "        best_corr_coefs.append(best_corr_coef)\n",
        "        print(f'Fold {fold + 1}: Best Correlation Coefficient: {best_corr_coef:.4f}')\n",
        "    average_corr_coef = np.mean(best_corr_coefs)\n",
        "    print(f\"\\n--- Training Complete ---\")\n",
        "    print(f\"Average Best Correlation Coefficient across all folds: {average_corr_coef:.4f}\")\n",
        "    time_end = time.time()\n",
        "    execution_time = int(time_end - time_star)\n",
        "    print(f\"Total execution time: {execution_time // 60} minutes, {execution_time % 60} seconds.\")\n",
        "    result_data = {'time': [execution_time], 'mean_corr_coef': [average_corr_coef]}\n",
        "    pd.DataFrame(result_data).to_csv(\"/content/drive/MyDrive/crop_former_model/final_training_results.csv\")\n",
        "\n",
        "def data_preprocessing(data_path, label_path, target_columns=10000):\n",
        "    label = pd.read_csv(label_path, index_col=0).values\n",
        "    data = pd.read_csv(data_path, index_col=0)\n",
        "    if data.shape[1] < target_columns:\n",
        "        missing_columns = target_columns - data.shape[1]\n",
        "        zeros_df = pd.DataFrame(np.zeros((data.shape[0], missing_columns)))\n",
        "        data = pd.concat([data, zeros_df], axis=1)\n",
        "    data = data.values\n",
        "    return data, label\n",
        "\n",
        "# ============================\n",
        "# 4. MAIN EXECUTION BLOCK\n",
        "# ============================\n",
        "if __name__ == '__main__':\n",
        "    # --- Hyperparameters ---\n",
        "    outer_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    inner_cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "    batch_size = 128\n",
        "    learning_rate = 0.001\n",
        "    patience = 5\n",
        "    hidden_dim = 64\n",
        "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    print(f\"Training on device: {DEVICE}\")\n",
        "\n",
        "    # --- File Paths ---\n",
        "    data_path = \"/content/drive/MyDrive/crop_former_model/X_train.csv\"\n",
        "    label_path = \"/content/drive/MyDrive/crop_former_model/y_train.csv\"\n",
        "\n",
        "    # --- Run ---\n",
        "    print(\"Starting data preprocessing...\")\n",
        "    data, label = data_preprocessing(data_path, label_path)\n",
        "    print(\"Data preprocessing complete. Starting training...\")\n",
        "    run_nested_cv_with_early_stopping(data=data, label=label, outer_cv=outer_cv,\n",
        "                                      inner_cv=inner_cv, learning_rate=learning_rate,\n",
        "                                      batch_size=batch_size, hidden_dim=hidden_dim, output_dim=1,\n",
        "                                      kernel_size=3, patience=patience, DEVICE=DEVICE)\n",
        "    print(\"--- Successfully Finished ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttO7SK6H6QxP",
        "outputId": "933ebac7-ab61-4eec-9a15-fd05b1e9e781"
      },
      "execution_count": 3,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training on device: cuda\n",
            "Starting data preprocessing...\n",
            "Data preprocessing complete. Starting training...\n",
            "\n",
            "--- Starting Fold 1/5 ---\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-04 07:38:26,208] A new study created in memory with name: no-name-9bbdd24d-4d63-4136-805d-45ce73b5e9c7\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running Optuna for hyperparameter tuning...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-04 07:38:44,091] Trial 0 finished with value: 353.6909993489583 and parameters: {'num_attention_heads': 8, 'attention_probs_dropout_prob': 0.5}. Best is trial 0 with value: 353.6909993489583.\n",
            "[I 2025-10-04 07:39:01,157] Trial 1 finished with value: 604.6578494340946 and parameters: {'num_attention_heads': 4, 'attention_probs_dropout_prob': 0.5}. Best is trial 0 with value: 353.6909993489583.\n",
            "[I 2025-10-04 07:39:19,048] Trial 2 finished with value: 517.586169642261 and parameters: {'num_attention_heads': 4, 'attention_probs_dropout_prob': 0.2}. Best is trial 0 with value: 353.6909993489583.\n",
            "[I 2025-10-04 07:39:36,890] Trial 3 finished with value: 398.65988863431494 and parameters: {'num_attention_heads': 4, 'attention_probs_dropout_prob': 0.5}. Best is trial 0 with value: 353.6909993489583.\n",
            "[I 2025-10-04 07:39:54,053] Trial 4 finished with value: 450.1202152610844 and parameters: {'num_attention_heads': 4, 'attention_probs_dropout_prob': 0.5}. Best is trial 0 with value: 353.6909993489583.\n",
            "[I 2025-10-04 07:40:11,330] Trial 5 finished with value: 521.1191043690739 and parameters: {'num_attention_heads': 4, 'attention_probs_dropout_prob': 0.5}. Best is trial 0 with value: 353.6909993489583.\n",
            "[I 2025-10-04 07:40:29,093] Trial 6 finished with value: 430.47161473983374 and parameters: {'num_attention_heads': 8, 'attention_probs_dropout_prob': 0.2}. Best is trial 0 with value: 353.6909993489583.\n",
            "[I 2025-10-04 07:40:46,136] Trial 7 finished with value: 406.71057989658453 and parameters: {'num_attention_heads': 4, 'attention_probs_dropout_prob': 0.5}. Best is trial 0 with value: 353.6909993489583.\n",
            "[I 2025-10-04 07:41:03,138] Trial 8 finished with value: 481.3790426661826 and parameters: {'num_attention_heads': 4, 'attention_probs_dropout_prob': 0.2}. Best is trial 0 with value: 353.6909993489583.\n",
            "[I 2025-10-04 07:41:20,928] Trial 9 finished with value: 435.8070642194177 and parameters: {'num_attention_heads': 4, 'attention_probs_dropout_prob': 0.2}. Best is trial 0 with value: 353.6909993489583.\n",
            "[I 2025-10-04 07:41:38,120] Trial 10 finished with value: 387.5682860806457 and parameters: {'num_attention_heads': 8, 'attention_probs_dropout_prob': 0.5}. Best is trial 0 with value: 353.6909993489583.\n",
            "[I 2025-10-04 07:41:55,272] Trial 11 finished with value: 514.3839703421307 and parameters: {'num_attention_heads': 8, 'attention_probs_dropout_prob': 0.5}. Best is trial 0 with value: 353.6909993489583.\n",
            "[I 2025-10-04 07:42:14,249] Trial 12 finished with value: 485.386976453993 and parameters: {'num_attention_heads': 8, 'attention_probs_dropout_prob': 0.5}. Best is trial 0 with value: 353.6909993489583.\n",
            "[I 2025-10-04 07:42:32,529] Trial 13 finished with value: 365.3354210486779 and parameters: {'num_attention_heads': 8, 'attention_probs_dropout_prob': 0.5}. Best is trial 0 with value: 353.6909993489583.\n",
            "[I 2025-10-04 07:42:50,426] Trial 14 finished with value: 389.62730890257745 and parameters: {'num_attention_heads': 8, 'attention_probs_dropout_prob': 0.5}. Best is trial 0 with value: 353.6909993489583.\n",
            "[I 2025-10-04 07:43:10,270] Trial 15 finished with value: 418.246847299429 and parameters: {'num_attention_heads': 8, 'attention_probs_dropout_prob': 0.5}. Best is trial 0 with value: 353.6909993489583.\n",
            "[I 2025-10-04 07:43:28,044] Trial 16 finished with value: 511.26389201889697 and parameters: {'num_attention_heads': 8, 'attention_probs_dropout_prob': 0.5}. Best is trial 0 with value: 353.6909993489583.\n",
            "[I 2025-10-04 07:43:45,884] Trial 17 finished with value: 616.1475788344684 and parameters: {'num_attention_heads': 8, 'attention_probs_dropout_prob': 0.5}. Best is trial 0 with value: 353.6909993489583.\n",
            "[I 2025-10-04 07:44:04,087] Trial 18 finished with value: 518.5871276855469 and parameters: {'num_attention_heads': 8, 'attention_probs_dropout_prob': 0.2}. Best is trial 0 with value: 353.6909993489583.\n",
            "[I 2025-10-04 07:44:21,743] Trial 19 finished with value: 535.5340565738514 and parameters: {'num_attention_heads': 8, 'attention_probs_dropout_prob': 0.5}. Best is trial 0 with value: 353.6909993489583.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optuna found best params for Fold 1: {'num_attention_heads': 8, 'attention_probs_dropout_prob': 0.5}\n",
            "Starting final training for Fold 1...\n",
            "Early stopping at epoch 20\n",
            "Fold 1: Best Correlation Coefficient: 0.8090\n",
            "\n",
            "--- Starting Fold 2/5 ---\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-04 07:44:26,039] A new study created in memory with name: no-name-e2b84bde-6e06-4b20-80a5-b8cd41d1a4c7\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running Optuna for hyperparameter tuning...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-04 07:44:44,116] Trial 0 finished with value: 489.9615627191006 and parameters: {'num_attention_heads': 8, 'attention_probs_dropout_prob': 0.2}. Best is trial 0 with value: 489.9615627191006.\n",
            "[I 2025-10-04 07:45:01,650] Trial 1 finished with value: 479.45655601045 and parameters: {'num_attention_heads': 4, 'attention_probs_dropout_prob': 0.5}. Best is trial 1 with value: 479.45655601045.\n",
            "[I 2025-10-04 07:45:21,275] Trial 2 finished with value: 413.1463390904614 and parameters: {'num_attention_heads': 8, 'attention_probs_dropout_prob': 0.5}. Best is trial 2 with value: 413.1463390904614.\n",
            "[I 2025-10-04 07:45:38,994] Trial 3 finished with value: 487.7657953246027 and parameters: {'num_attention_heads': 4, 'attention_probs_dropout_prob': 0.2}. Best is trial 2 with value: 413.1463390904614.\n",
            "[I 2025-10-04 07:45:56,629] Trial 4 finished with value: 390.6503024631076 and parameters: {'num_attention_heads': 8, 'attention_probs_dropout_prob': 0.5}. Best is trial 4 with value: 390.6503024631076.\n",
            "[I 2025-10-04 07:46:15,090] Trial 5 finished with value: 551.6702904334435 and parameters: {'num_attention_heads': 4, 'attention_probs_dropout_prob': 0.5}. Best is trial 4 with value: 390.6503024631076.\n",
            "[I 2025-10-04 07:46:32,724] Trial 6 finished with value: 432.2178670769063 and parameters: {'num_attention_heads': 4, 'attention_probs_dropout_prob': 0.2}. Best is trial 4 with value: 390.6503024631076.\n",
            "[I 2025-10-04 07:46:50,562] Trial 7 finished with value: 403.1733299320579 and parameters: {'num_attention_heads': 8, 'attention_probs_dropout_prob': 0.2}. Best is trial 4 with value: 390.6503024631076.\n",
            "[I 2025-10-04 07:47:08,761] Trial 8 finished with value: 511.4407166049012 and parameters: {'num_attention_heads': 4, 'attention_probs_dropout_prob': 0.2}. Best is trial 4 with value: 390.6503024631076.\n",
            "[I 2025-10-04 07:47:26,626] Trial 9 finished with value: 479.41947115384613 and parameters: {'num_attention_heads': 8, 'attention_probs_dropout_prob': 0.2}. Best is trial 4 with value: 390.6503024631076.\n",
            "[I 2025-10-04 07:47:47,684] Trial 10 finished with value: 397.7755300407735 and parameters: {'num_attention_heads': 8, 'attention_probs_dropout_prob': 0.5}. Best is trial 4 with value: 390.6503024631076.\n",
            "[I 2025-10-04 07:48:07,869] Trial 11 finished with value: 448.4194781963642 and parameters: {'num_attention_heads': 8, 'attention_probs_dropout_prob': 0.5}. Best is trial 4 with value: 390.6503024631076.\n",
            "[I 2025-10-04 07:48:26,473] Trial 12 finished with value: 381.82404946873334 and parameters: {'num_attention_heads': 8, 'attention_probs_dropout_prob': 0.5}. Best is trial 12 with value: 381.82404946873334.\n",
            "[I 2025-10-04 07:48:45,074] Trial 13 finished with value: 429.75497149606036 and parameters: {'num_attention_heads': 8, 'attention_probs_dropout_prob': 0.5}. Best is trial 12 with value: 381.82404946873334.\n",
            "[I 2025-10-04 07:49:02,956] Trial 14 finished with value: 577.1024905473759 and parameters: {'num_attention_heads': 8, 'attention_probs_dropout_prob': 0.5}. Best is trial 12 with value: 381.82404946873334.\n",
            "[I 2025-10-04 07:49:21,058] Trial 15 finished with value: 514.938250941089 and parameters: {'num_attention_heads': 8, 'attention_probs_dropout_prob': 0.5}. Best is trial 12 with value: 381.82404946873334.\n",
            "[I 2025-10-04 07:49:38,841] Trial 16 finished with value: 496.766589303302 and parameters: {'num_attention_heads': 8, 'attention_probs_dropout_prob': 0.5}. Best is trial 12 with value: 381.82404946873334.\n",
            "[I 2025-10-04 07:49:56,695] Trial 17 finished with value: 445.8559713771201 and parameters: {'num_attention_heads': 8, 'attention_probs_dropout_prob': 0.5}. Best is trial 12 with value: 381.82404946873334.\n",
            "[I 2025-10-04 07:50:15,199] Trial 18 finished with value: 447.36085236378204 and parameters: {'num_attention_heads': 8, 'attention_probs_dropout_prob': 0.5}. Best is trial 12 with value: 381.82404946873334.\n",
            "[I 2025-10-04 07:50:32,811] Trial 19 finished with value: 348.3534258981037 and parameters: {'num_attention_heads': 8, 'attention_probs_dropout_prob': 0.5}. Best is trial 19 with value: 348.3534258981037.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optuna found best params for Fold 2: {'num_attention_heads': 8, 'attention_probs_dropout_prob': 0.5}\n",
            "Starting final training for Fold 2...\n",
            "Early stopping at epoch 75\n",
            "Fold 2: Best Correlation Coefficient: 0.9417\n",
            "\n",
            "--- Starting Fold 3/5 ---\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-04 07:50:46,500] A new study created in memory with name: no-name-e1a12e1c-02b5-4702-b363-d25449a4a4be\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running Optuna for hyperparameter tuning...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-04 07:51:03,959] Trial 0 finished with value: 597.2101962097689 and parameters: {'num_attention_heads': 4, 'attention_probs_dropout_prob': 0.2}. Best is trial 0 with value: 597.2101962097689.\n",
            "[I 2025-10-04 07:51:22,896] Trial 1 finished with value: 504.1681951539129 and parameters: {'num_attention_heads': 4, 'attention_probs_dropout_prob': 0.2}. Best is trial 1 with value: 504.1681951539129.\n",
            "[I 2025-10-04 07:51:41,093] Trial 2 finished with value: 486.88317818926953 and parameters: {'num_attention_heads': 4, 'attention_probs_dropout_prob': 0.2}. Best is trial 2 with value: 486.88317818926953.\n",
            "[I 2025-10-04 07:52:01,675] Trial 3 finished with value: 497.7334722535223 and parameters: {'num_attention_heads': 8, 'attention_probs_dropout_prob': 0.2}. Best is trial 2 with value: 486.88317818926953.\n",
            "[I 2025-10-04 07:52:20,822] Trial 4 finished with value: 504.28173932458594 and parameters: {'num_attention_heads': 8, 'attention_probs_dropout_prob': 0.2}. Best is trial 2 with value: 486.88317818926953.\n",
            "[I 2025-10-04 07:52:40,798] Trial 5 finished with value: 381.46028384999335 and parameters: {'num_attention_heads': 4, 'attention_probs_dropout_prob': 0.2}. Best is trial 5 with value: 381.46028384999335.\n",
            "[I 2025-10-04 07:53:00,201] Trial 6 finished with value: 352.16737352680957 and parameters: {'num_attention_heads': 4, 'attention_probs_dropout_prob': 0.5}. Best is trial 6 with value: 352.16737352680957.\n",
            "[I 2025-10-04 07:53:19,467] Trial 7 finished with value: 534.0267894777477 and parameters: {'num_attention_heads': 8, 'attention_probs_dropout_prob': 0.2}. Best is trial 6 with value: 352.16737352680957.\n",
            "[I 2025-10-04 07:53:39,305] Trial 8 finished with value: 493.5998940753122 and parameters: {'num_attention_heads': 4, 'attention_probs_dropout_prob': 0.5}. Best is trial 6 with value: 352.16737352680957.\n",
            "[I 2025-10-04 07:53:58,213] Trial 9 finished with value: 353.06766620864215 and parameters: {'num_attention_heads': 8, 'attention_probs_dropout_prob': 0.5}. Best is trial 6 with value: 352.16737352680957.\n",
            "[I 2025-10-04 07:54:19,289] Trial 10 finished with value: 558.1133180275941 and parameters: {'num_attention_heads': 4, 'attention_probs_dropout_prob': 0.5}. Best is trial 6 with value: 352.16737352680957.\n",
            "[I 2025-10-04 07:54:38,628] Trial 11 finished with value: 401.26821325579255 and parameters: {'num_attention_heads': 8, 'attention_probs_dropout_prob': 0.5}. Best is trial 6 with value: 352.16737352680957.\n",
            "[I 2025-10-04 07:54:58,885] Trial 12 finished with value: 384.2641460712139 and parameters: {'num_attention_heads': 8, 'attention_probs_dropout_prob': 0.5}. Best is trial 6 with value: 352.16737352680957.\n",
            "[I 2025-10-04 07:55:18,219] Trial 13 finished with value: 407.9166999230018 and parameters: {'num_attention_heads': 8, 'attention_probs_dropout_prob': 0.5}. Best is trial 6 with value: 352.16737352680957.\n",
            "[I 2025-10-04 07:55:38,626] Trial 14 finished with value: 421.4913074460804 and parameters: {'num_attention_heads': 4, 'attention_probs_dropout_prob': 0.5}. Best is trial 6 with value: 352.16737352680957.\n",
            "[I 2025-10-04 07:55:58,345] Trial 15 finished with value: 525.1316157936031 and parameters: {'num_attention_heads': 8, 'attention_probs_dropout_prob': 0.5}. Best is trial 6 with value: 352.16737352680957.\n",
            "[I 2025-10-04 07:56:18,871] Trial 16 finished with value: 491.23573055430353 and parameters: {'num_attention_heads': 8, 'attention_probs_dropout_prob': 0.5}. Best is trial 6 with value: 352.16737352680957.\n",
            "[I 2025-10-04 07:56:37,750] Trial 17 finished with value: 391.5981244470319 and parameters: {'num_attention_heads': 4, 'attention_probs_dropout_prob': 0.5}. Best is trial 6 with value: 352.16737352680957.\n",
            "[I 2025-10-04 07:56:57,123] Trial 18 finished with value: 423.30415174696174 and parameters: {'num_attention_heads': 8, 'attention_probs_dropout_prob': 0.5}. Best is trial 6 with value: 352.16737352680957.\n",
            "[I 2025-10-04 07:57:17,749] Trial 19 finished with value: 470.9815645136385 and parameters: {'num_attention_heads': 4, 'attention_probs_dropout_prob': 0.5}. Best is trial 6 with value: 352.16737352680957.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optuna found best params for Fold 3: {'num_attention_heads': 4, 'attention_probs_dropout_prob': 0.5}\n",
            "Starting final training for Fold 3...\n",
            "Early stopping at epoch 49\n",
            "Fold 3: Best Correlation Coefficient: 0.9363\n",
            "\n",
            "--- Starting Fold 4/5 ---\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-04 07:57:28,489] A new study created in memory with name: no-name-3e5eb089-75c1-492c-aaf8-1afa13457088\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running Optuna for hyperparameter tuning...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-04 07:57:46,861] Trial 0 finished with value: 477.2264190412994 and parameters: {'num_attention_heads': 4, 'attention_probs_dropout_prob': 0.2}. Best is trial 0 with value: 477.2264190412994.\n",
            "[I 2025-10-04 07:58:04,437] Trial 1 finished with value: 586.4726421649639 and parameters: {'num_attention_heads': 4, 'attention_probs_dropout_prob': 0.2}. Best is trial 0 with value: 477.2264190412994.\n",
            "[I 2025-10-04 07:58:22,779] Trial 2 finished with value: 477.21240495209 and parameters: {'num_attention_heads': 4, 'attention_probs_dropout_prob': 0.2}. Best is trial 2 with value: 477.21240495209.\n",
            "[I 2025-10-04 07:58:40,104] Trial 3 finished with value: 498.09606868385254 and parameters: {'num_attention_heads': 4, 'attention_probs_dropout_prob': 0.2}. Best is trial 2 with value: 477.21240495209.\n",
            "[I 2025-10-04 07:58:57,394] Trial 4 finished with value: 502.6407564603366 and parameters: {'num_attention_heads': 4, 'attention_probs_dropout_prob': 0.5}. Best is trial 2 with value: 477.21240495209.\n",
            "[I 2025-10-04 07:59:15,401] Trial 5 finished with value: 408.3650387494992 and parameters: {'num_attention_heads': 8, 'attention_probs_dropout_prob': 0.2}. Best is trial 5 with value: 408.3650387494992.\n",
            "[I 2025-10-04 07:59:32,761] Trial 6 finished with value: 540.0834606203258 and parameters: {'num_attention_heads': 8, 'attention_probs_dropout_prob': 0.5}. Best is trial 5 with value: 408.3650387494992.\n",
            "[I 2025-10-04 07:59:50,754] Trial 7 finished with value: 573.6566256009615 and parameters: {'num_attention_heads': 4, 'attention_probs_dropout_prob': 0.2}. Best is trial 5 with value: 408.3650387494992.\n",
            "[I 2025-10-04 08:00:11,650] Trial 8 finished with value: 318.3359140249399 and parameters: {'num_attention_heads': 8, 'attention_probs_dropout_prob': 0.5}. Best is trial 8 with value: 318.3359140249399.\n",
            "[I 2025-10-04 08:00:29,453] Trial 9 finished with value: 439.08042477338745 and parameters: {'num_attention_heads': 4, 'attention_probs_dropout_prob': 0.2}. Best is trial 8 with value: 318.3359140249399.\n",
            "[I 2025-10-04 08:00:47,681] Trial 10 finished with value: 305.33795609433423 and parameters: {'num_attention_heads': 8, 'attention_probs_dropout_prob': 0.5}. Best is trial 10 with value: 305.33795609433423.\n",
            "[I 2025-10-04 08:01:06,048] Trial 11 finished with value: 426.3568386501736 and parameters: {'num_attention_heads': 8, 'attention_probs_dropout_prob': 0.5}. Best is trial 10 with value: 305.33795609433423.\n",
            "[I 2025-10-04 08:01:24,252] Trial 12 finished with value: 543.1947373610277 and parameters: {'num_attention_heads': 8, 'attention_probs_dropout_prob': 0.5}. Best is trial 10 with value: 305.33795609433423.\n",
            "[I 2025-10-04 08:01:42,388] Trial 13 finished with value: 325.6581913874699 and parameters: {'num_attention_heads': 8, 'attention_probs_dropout_prob': 0.5}. Best is trial 10 with value: 305.33795609433423.\n",
            "[I 2025-10-04 08:01:59,781] Trial 14 finished with value: 470.4812322111211 and parameters: {'num_attention_heads': 8, 'attention_probs_dropout_prob': 0.5}. Best is trial 10 with value: 305.33795609433423.\n",
            "[I 2025-10-04 08:02:17,423] Trial 15 finished with value: 407.3365992358608 and parameters: {'num_attention_heads': 8, 'attention_probs_dropout_prob': 0.5}. Best is trial 10 with value: 305.33795609433423.\n",
            "[I 2025-10-04 08:02:35,630] Trial 16 finished with value: 403.573978261051 and parameters: {'num_attention_heads': 8, 'attention_probs_dropout_prob': 0.5}. Best is trial 10 with value: 305.33795609433423.\n",
            "[I 2025-10-04 08:02:53,045] Trial 17 finished with value: 427.0821209768964 and parameters: {'num_attention_heads': 8, 'attention_probs_dropout_prob': 0.5}. Best is trial 10 with value: 305.33795609433423.\n",
            "[I 2025-10-04 08:03:11,698] Trial 18 finished with value: 493.1061583950989 and parameters: {'num_attention_heads': 8, 'attention_probs_dropout_prob': 0.5}. Best is trial 10 with value: 305.33795609433423.\n",
            "[I 2025-10-04 08:03:30,127] Trial 19 finished with value: 373.1530354817708 and parameters: {'num_attention_heads': 8, 'attention_probs_dropout_prob': 0.5}. Best is trial 10 with value: 305.33795609433423.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optuna found best params for Fold 4: {'num_attention_heads': 8, 'attention_probs_dropout_prob': 0.5}\n",
            "Starting final training for Fold 4...\n",
            "Early stopping at epoch 47\n",
            "Fold 4: Best Correlation Coefficient: 0.9427\n",
            "\n",
            "--- Starting Fold 5/5 ---\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-04 08:03:39,560] A new study created in memory with name: no-name-fbc54b1c-ec33-4f8a-9688-69c382ca5d18\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running Optuna for hyperparameter tuning...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-10-04 08:03:57,250] Trial 0 finished with value: 415.24580292008886 and parameters: {'num_attention_heads': 8, 'attention_probs_dropout_prob': 0.2}. Best is trial 0 with value: 415.24580292008886.\n",
            "[I 2025-10-04 08:04:14,436] Trial 1 finished with value: 519.2561893300114 and parameters: {'num_attention_heads': 4, 'attention_probs_dropout_prob': 0.2}. Best is trial 0 with value: 415.24580292008886.\n",
            "[I 2025-10-04 08:04:32,216] Trial 2 finished with value: 577.575193225828 and parameters: {'num_attention_heads': 8, 'attention_probs_dropout_prob': 0.5}. Best is trial 0 with value: 415.24580292008886.\n",
            "[I 2025-10-04 08:04:49,559] Trial 3 finished with value: 379.4198857494909 and parameters: {'num_attention_heads': 4, 'attention_probs_dropout_prob': 0.5}. Best is trial 3 with value: 379.4198857494909.\n",
            "[I 2025-10-04 08:05:06,679] Trial 4 finished with value: 332.700180314545 and parameters: {'num_attention_heads': 4, 'attention_probs_dropout_prob': 0.5}. Best is trial 4 with value: 332.700180314545.\n",
            "[I 2025-10-04 08:05:24,427] Trial 5 finished with value: 457.22799291366186 and parameters: {'num_attention_heads': 8, 'attention_probs_dropout_prob': 0.2}. Best is trial 4 with value: 332.700180314545.\n",
            "[I 2025-10-04 08:05:41,866] Trial 6 finished with value: 459.86162990993927 and parameters: {'num_attention_heads': 4, 'attention_probs_dropout_prob': 0.5}. Best is trial 4 with value: 332.700180314545.\n",
            "[I 2025-10-04 08:05:59,134] Trial 7 finished with value: 481.19425351395574 and parameters: {'num_attention_heads': 4, 'attention_probs_dropout_prob': 0.2}. Best is trial 4 with value: 332.700180314545.\n",
            "[I 2025-10-04 08:06:17,496] Trial 8 finished with value: 597.591797918336 and parameters: {'num_attention_heads': 4, 'attention_probs_dropout_prob': 0.2}. Best is trial 4 with value: 332.700180314545.\n",
            "[I 2025-10-04 08:06:34,528] Trial 9 finished with value: 600.6028567583134 and parameters: {'num_attention_heads': 4, 'attention_probs_dropout_prob': 0.2}. Best is trial 4 with value: 332.700180314545.\n",
            "[I 2025-10-04 08:06:51,506] Trial 10 finished with value: 422.2112846700554 and parameters: {'num_attention_heads': 8, 'attention_probs_dropout_prob': 0.5}. Best is trial 4 with value: 332.700180314545.\n",
            "[I 2025-10-04 08:07:09,016] Trial 11 finished with value: 313.2218058007395 and parameters: {'num_attention_heads': 4, 'attention_probs_dropout_prob': 0.5}. Best is trial 11 with value: 313.2218058007395.\n",
            "[I 2025-10-04 08:07:26,180] Trial 12 finished with value: 489.22717650323847 and parameters: {'num_attention_heads': 4, 'attention_probs_dropout_prob': 0.5}. Best is trial 11 with value: 313.2218058007395.\n",
            "[I 2025-10-04 08:07:43,230] Trial 13 finished with value: 396.68307234283185 and parameters: {'num_attention_heads': 4, 'attention_probs_dropout_prob': 0.5}. Best is trial 11 with value: 313.2218058007395.\n",
            "[I 2025-10-04 08:08:00,657] Trial 14 finished with value: 428.4028680263421 and parameters: {'num_attention_heads': 4, 'attention_probs_dropout_prob': 0.5}. Best is trial 11 with value: 313.2218058007395.\n",
            "[I 2025-10-04 08:08:18,022] Trial 15 finished with value: 529.4387632190671 and parameters: {'num_attention_heads': 4, 'attention_probs_dropout_prob': 0.5}. Best is trial 11 with value: 313.2218058007395.\n",
            "[I 2025-10-04 08:08:35,196] Trial 16 finished with value: 501.5911690475594 and parameters: {'num_attention_heads': 4, 'attention_probs_dropout_prob': 0.5}. Best is trial 11 with value: 313.2218058007395.\n",
            "[I 2025-10-04 08:08:52,443] Trial 17 finished with value: 271.3217187865168 and parameters: {'num_attention_heads': 4, 'attention_probs_dropout_prob': 0.5}. Best is trial 17 with value: 271.3217187865168.\n",
            "[I 2025-10-04 08:09:09,980] Trial 18 finished with value: 551.6003624027611 and parameters: {'num_attention_heads': 8, 'attention_probs_dropout_prob': 0.5}. Best is trial 17 with value: 271.3217187865168.\n",
            "[I 2025-10-04 08:09:27,750] Trial 19 finished with value: 394.0548378708016 and parameters: {'num_attention_heads': 4, 'attention_probs_dropout_prob': 0.5}. Best is trial 17 with value: 271.3217187865168.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optuna found best params for Fold 5: {'num_attention_heads': 4, 'attention_probs_dropout_prob': 0.5}\n",
            "Starting final training for Fold 5...\n",
            "Early stopping at epoch 29\n",
            "Fold 5: Best Correlation Coefficient: 0.9277\n",
            "\n",
            "--- Training Complete ---\n",
            "Average Best Correlation Coefficient across all folds: 0.9115\n",
            "Total execution time: 31 minutes, 7 seconds.\n",
            "--- Successfully Finished ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch.nn as nn\n",
        "from lightning.pytorch import LightningModule\n",
        "\n",
        "# (The model definition is copied here for the script to be self-contained)\n",
        "class SelfAttention(LightningModule):\n",
        "    def __init__(self, num_attention_heads, input_size, hidden_size, output_dim=1, kernel_size=3,\n",
        "                 hidden_dropout_prob=0.5, attention_probs_dropout_prob=0.5, learning_rate=0.001):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.num_attention_heads = num_attention_heads\n",
        "        self.attention_head_size = int(hidden_size / num_attention_heads)\n",
        "        self.all_head_size = hidden_size\n",
        "        self.query = torch.nn.Linear(input_size, self.all_head_size)\n",
        "        self.key = torch.nn.Linear(input_size, self.all_head_size)\n",
        "        self.value = torch.nn.Linear(input_size, self.all_head_size)\n",
        "        self.attn_dropout = torch.nn.Dropout(attention_probs_dropout_prob)\n",
        "        self.out_dropout = torch.nn.Dropout(hidden_dropout_prob)\n",
        "        self.dense = torch.nn.Linear(hidden_size, input_size)\n",
        "        self.LayerNorm = torch.nn.LayerNorm(input_size, eps=1e-12)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.out = torch.nn.Linear(input_size, output_dim)\n",
        "        self.cnn = torch.nn.Conv1d(1, 1, kernel_size, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        cnn_hidden = self.cnn(input_tensor.view(input_tensor.size(0), 1, -1))\n",
        "        input_tensor_after_cnn = cnn_hidden\n",
        "        mixed_query_layer = self.query(input_tensor_after_cnn)\n",
        "        mixed_key_layer = self.key(input_tensor_after_cnn)\n",
        "        mixed_value_layer = self.value(input_tensor_after_cnn)\n",
        "        query_layer, key_layer, value_layer = mixed_query_layer, mixed_key_layer, mixed_value_layer\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "        attention_scores = attention_scores / np.sqrt(self.attention_head_size)\n",
        "        attention_probs = torch.nn.Softmax(dim=-1)(attention_scores)\n",
        "        attention_probs = self.attn_dropout(attention_probs)\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "        hidden_states = self.dense(context_layer)\n",
        "        hidden_states = self.out_dropout(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor_after_cnn)\n",
        "        output = self.out(self.relu(hidden_states.view(hidden_states.size(0), -1)))\n",
        "        return output\n",
        "\n",
        "# --- Main Prediction Logic ---\n",
        "if __name__ == '__main__':\n",
        "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f'Using device: {DEVICE}')\n",
        "\n",
        "    # --- UPDATE THESE FILE PATHS ---\n",
        "    # Use the model from the fold with the highest score (e.g., Fold 5)\n",
        "    model_path = '/content/drive/MyDrive/crop_former_model/best_model_fold_5.pth'\n",
        "\n",
        "    # We need the training data to fit the scaler correctly\n",
        "    train_data_path = '/content/drive/MyDrive/crop_former_model/X_train.csv'\n",
        "\n",
        "    # This is the \"new\" data we want to predict on\n",
        "    test_data_path = '/content/drive/MyDrive/crop_former_model/X_test.csv'\n",
        "\n",
        "    # This is where the final predictions will be saved\n",
        "    output_path = '/content/drive/MyDrive/crop_former_model/predicted_result.csv'\n",
        "    # --------------------------------\n",
        "\n",
        "    # --- Hyperparameters (must match the trained model) ---\n",
        "    input_size = 10000\n",
        "    # Use the best params found for your best fold.\n",
        "    # From your log, Fold 5 used: {'num_attention_heads': 4, 'attention_probs_dropout_prob': 0.5}\n",
        "    num_attention_heads = 4\n",
        "    attention_probs_dropout_prob = 0.5\n",
        "\n",
        "    # Initialize the model structure\n",
        "    model = SelfAttention(num_attention_heads=num_attention_heads, input_size=input_size,\n",
        "                          hidden_size=64, output_dim=1, kernel_size=3,\n",
        "                          attention_probs_dropout_prob=attention_probs_dropout_prob).to(DEVICE)\n",
        "\n",
        "    # Load the saved weights\n",
        "    print(f\"Loading trained model from: {model_path}\")\n",
        "    model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
        "\n",
        "    # --- CRITICAL FIX: SCALING THE DATA ---\n",
        "    print(\"Loading training data to fit the scaler...\")\n",
        "    X_train = pd.read_csv(train_data_path, index_col=0)\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(X_train.values) # Fit the scaler ONLY on the training data\n",
        "\n",
        "    print(f\"Loading and scaling test data from: {test_data_path}\")\n",
        "    X_test = pd.read_csv(test_data_path, index_col=0)\n",
        "    X_test_scaled = scaler.transform(X_test.values) # Transform the test data\n",
        "    # -----------------------------------------\n",
        "\n",
        "    X_test_tensor = torch.from_numpy(X_test_scaled).to(torch.float32).to(DEVICE)\n",
        "\n",
        "    # Make predictions\n",
        "    print(\"Making predictions on the test data...\")\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        output = model(X_test_tensor)\n",
        "\n",
        "    # Save predictions\n",
        "    pd.DataFrame(output.cpu().numpy(), columns=['predicted_value'], index=X_test.index).to_csv(output_path)\n",
        "\n",
        "    print(f\"✅ Success! Predictions saved to '{output_path}'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdOk-IEJ7Z2T",
        "outputId": "e734e06e-4285-47ce-bf9a-00a389dbd939"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Loading trained model from: /content/drive/MyDrive/crop_former_model/best_model_fold_5.pth\n",
            "Loading training data to fit the scaler...\n",
            "Loading and scaling test data from: /content/drive/MyDrive/crop_former_model/X_test.csv\n",
            "Making predictions on the test data...\n",
            "✅ Success! Predictions saved to '/content/drive/MyDrive/crop_former_model/predicted_result.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# --- IMPORTANT: UPDATE THESE FILE PATHS ---\n",
        "# Path to the file with your model's predictions\n",
        "predicted_file = '/content/drive/MyDrive/crop_former_model/predicted_result.csv'\n",
        "\n",
        "# Path to the file with the true, original test labels\n",
        "true_labels_file = '/content/drive/MyDrive/crop_former_model/y_test.csv'\n",
        "# -----------------------------------------\n",
        "\n",
        "# Load the datasets\n",
        "predicted_df = pd.read_csv(predicted_file)\n",
        "true_df = pd.read_csv(true_labels_file)\n",
        "\n",
        "# Extract the numerical values\n",
        "predicted_values = predicted_df['predicted_value'].values\n",
        "true_values = true_df.iloc[:, 1].values # Select the second column which contains the phenotype values\n",
        "\n",
        "# Calculate the Pearson Correlation Coefficient (PCC)\n",
        "# np.corrcoef returns a 2x2 matrix, the value at [0, 1] is the correlation\n",
        "accuracy = np.corrcoef(predicted_values, true_values)[0, 1]\n",
        "\n",
        "print(f\"✅ Model Accuracy (Pearson Correlation Coefficient): {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByuJj3-vSwO5",
        "outputId": "d34717e8-69f4-4616-d309-49d47f22960f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model Accuracy (Pearson Correlation Coefficient): 0.9285\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# --- Configuration ---\n",
        "# File with the correct 10,000 SNP columns\n",
        "feature_file = '/content/drive/MyDrive/crop_former_model/chr10_top10k_snps.csv'\n",
        "\n",
        "# Name of our new simulated data file\n",
        "output_simulated_file = '/content/drive/MyDrive/crop_former_model/new_simulated_maize.csv'\n",
        "\n",
        "# How many new samples to simulate\n",
        "num_new_samples = 5\n",
        "# ---------------------\n",
        "\n",
        "print(\"Creating a simulated data file for prediction...\")\n",
        "\n",
        "# Read just the header of the feature file to get the column names\n",
        "try:\n",
        "    snp_columns = pd.read_csv(feature_file, nrows=0).columns[1:] # Skip the first 'IID' column\n",
        "\n",
        "    # Create some new, fake sample IDs\n",
        "    new_sample_ids = [f'Maize_Sample_{i+1}' for i in range(num_new_samples)]\n",
        "\n",
        "    # Create a DataFrame with the correct shape\n",
        "    simulated_df = pd.DataFrame(index=new_sample_ids, columns=snp_columns)\n",
        "    simulated_df.index.name = 'IID'\n",
        "\n",
        "    # Fill it with random genotype data (the 0-9 encoding)\n",
        "    # This simulates having new genotype data for the same SNPs\n",
        "    simulated_data = np.random.randint(0, 10, size=simulated_df.shape)\n",
        "    simulated_df[:] = simulated_data\n",
        "\n",
        "    # Save the simulated data to a new CSV file\n",
        "    simulated_df.to_csv(output_simulated_file)\n",
        "\n",
        "    print(f\"✅ Success! Created '{output_simulated_file}' with {num_new_samples} simulated samples.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"❌ Error: Make sure '{feature_file}' exists in your Google Drive.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZvq_ZbkUNt8",
        "outputId": "979803c7-c69f-4962-fc96-fc787b773b2c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating a simulated data file for prediction...\n",
            "✅ Success! Created '/content/drive/MyDrive/crop_former_model/new_simulated_maize.csv' with 5 simulated samples.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t"
      ],
      "metadata": {
        "id": "NiH1cL0zXsie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# Master Prediction and Evaluation Script for Cropformer\n",
        "# ===================================================================\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch.nn as nn\n",
        "from lightning.pytorch import LightningModule\n",
        "import os\n",
        "\n",
        "# ========================\n",
        "# 1. MODEL DEFINITION\n",
        "# (This must be included so the script knows the model's structure)\n",
        "# ========================\n",
        "class SelfAttention(LightningModule):\n",
        "    def __init__(self, num_attention_heads, input_size, hidden_size, output_dim=1, kernel_size=3,\n",
        "                 hidden_dropout_prob=0.5, attention_probs_dropout_prob=0.5):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.num_attention_heads = num_attention_heads\n",
        "        self.attention_head_size = int(hidden_size / num_attention_heads)\n",
        "        self.all_head_size = hidden_size\n",
        "        self.query = torch.nn.Linear(input_size, self.all_head_size)\n",
        "        self.key = torch.nn.Linear(input_size, self.all_head_size)\n",
        "        self.value = torch.nn.Linear(input_size, self.all_head_size)\n",
        "        self.attn_dropout = torch.nn.Dropout(attention_probs_dropout_prob)\n",
        "        self.out_dropout = torch.nn.Dropout(hidden_dropout_prob)\n",
        "        self.dense = torch.nn.Linear(hidden_size, input_size)\n",
        "        self.LayerNorm = torch.nn.LayerNorm(input_size, eps=1e-12)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.out = torch.nn.Linear(input_size, output_dim)\n",
        "        self.cnn = torch.nn.Conv1d(1, 1, kernel_size, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        cnn_hidden = self.cnn(input_tensor.view(input_tensor.size(0), 1, -1))\n",
        "        input_tensor_after_cnn = cnn_hidden\n",
        "        mixed_query_layer = self.query(input_tensor_after_cnn)\n",
        "        mixed_key_layer = self.key(input_tensor_after_cnn)\n",
        "        mixed_value_layer = self.value(input_tensor_after_cnn)\n",
        "        query_layer, key_layer, value_layer = mixed_query_layer, mixed_key_layer, mixed_value_layer\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "        attention_scores = attention_scores / np.sqrt(self.attention_head_size)\n",
        "        attention_probs = torch.nn.Softmax(dim=-1)(attention_scores)\n",
        "        attention_probs = self.attn_dropout(attention_probs)\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "        hidden_states = self.dense(context_layer)\n",
        "        hidden_states = self.out_dropout(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor_after_cnn)\n",
        "        output = self.out(self.relu(hidden_states.view(hidden_states.size(0), -1)))\n",
        "        return output\n",
        "\n",
        "# ========================\n",
        "# 2. CONFIGURATION\n",
        "# (Update these paths to match your Google Drive)\n",
        "# ========================\n",
        "DRIVE_FOLDER = '/content/drive/MyDrive/crop_former_model/'\n",
        "MODEL_FOLDER = DRIVE_FOLDER\n",
        "\n",
        "# Input files\n",
        "TRAIN_DATA_PATH = os.path.join(DRIVE_FOLDER, 'X_train.csv')\n",
        "TEST_DATA_PATH = os.path.join(DRIVE_FOLDER, 'X_test.csv')\n",
        "TEST_LABELS_PATH = os.path.join(DRIVE_FOLDER, 'y_test.csv')\n",
        "NEW_DATA_PATH = os.path.join(DRIVE_FOLDER, 'new_simulated_maize.csv')\n",
        "\n",
        "# --- Hyperparameters (must match the models you trained) ---\n",
        "INPUT_SIZE = 10000\n",
        "HIDDEN_SIZE = 64\n",
        "\n",
        "# ========================\n",
        "# 3. MAIN SCRIPT LOGIC\n",
        "# ========================\n",
        "if __name__ == '__main__':\n",
        "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f'Using device: {DEVICE}\\n')\n",
        "\n",
        "    # --- Part 1: Evaluate the accuracy of each saved model ---\n",
        "    print(\"--- Evaluating Accuracy of Saved Models ---\")\n",
        "\n",
        "    # Load data for evaluation\n",
        "    X_train_df = pd.read_csv(TRAIN_DATA_PATH, index_col=0)\n",
        "    X_test_df = pd.read_csv(TEST_DATA_PATH, index_col=0)\n",
        "    y_test_df = pd.read_csv(TEST_LABELS_PATH, index_col=0)\n",
        "\n",
        "    # Fit a scaler on the training data\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(X_train_df.values)\n",
        "\n",
        "    # Scale the test data\n",
        "    X_test_scaled = scaler.transform(X_test_df.values)\n",
        "    X_test_tensor = torch.from_numpy(X_test_scaled).to(torch.float32).to(DEVICE)\n",
        "    true_values = y_test_df.values.flatten()\n",
        "\n",
        "    model_accuracies = {}\n",
        "\n",
        "    for i in range(1, 6):\n",
        "        model_name = f'best_model_fold_{i}.pth'\n",
        "        model_path = os.path.join(MODEL_FOLDER, model_name)\n",
        "\n",
        "        # NOTE: This assumes best params are similar across folds. For highest accuracy,\n",
        "        # you'd need to load the specific params for each fold. We use the best overall.\n",
        "        best_params = {'num_attention_heads': 4, 'attention_probs_dropout_prob': 0.5}\n",
        "\n",
        "        model = SelfAttention(**best_params, input_size=INPUT_SIZE, hidden_size=HIDDEN_SIZE).to(DEVICE)\n",
        "        model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
        "        model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            predictions_tensor = model(X_test_tensor)\n",
        "\n",
        "        predicted_values = predictions_tensor.cpu().numpy().flatten()\n",
        "\n",
        "        # Calculate Pearson Correlation\n",
        "        accuracy = np.corrcoef(predicted_values, true_values)[0, 1]\n",
        "        model_accuracies[model_name] = accuracy\n",
        "        print(f\"  - {model_name}: Accuracy = {accuracy:.4f}\")\n",
        "\n",
        "    print(f\"\\nAverage Accuracy: {np.mean(list(model_accuracies.values())):.4f}\\n\")\n",
        "\n",
        "    # --- Part 2: Predict DTT for new data ---\n",
        "    print(f\"--- Predicting DTT for New Data from '{os.path.basename(NEW_DATA_PATH)}' ---\")\n",
        "\n",
        "    # Load the new data\n",
        "    new_data_df = pd.read_csv(NEW_DATA_PATH, index_col=0)\n",
        "\n",
        "    # Scale the new data using the SAME scaler fitted on the training data\n",
        "    new_data_scaled = scaler.transform(new_data_df.values)\n",
        "    new_data_tensor = torch.from_numpy(new_data_scaled).to(torch.float32).to(DEVICE)\n",
        "\n",
        "    all_predictions = {}\n",
        "\n",
        "    for i in range(1, 6):\n",
        "        model_name = f'best_model_fold_{i}.pth'\n",
        "        model_path = os.path.join(MODEL_FOLDER, model_name)\n",
        "\n",
        "        best_params = {'num_attention_heads': 4, 'attention_probs_dropout_prob': 0.5}\n",
        "\n",
        "        model = SelfAttention(**best_params, input_size=INPUT_SIZE, hidden_size=HIDDEN_SIZE).to(DEVICE)\n",
        "        model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
        "        model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            predictions_tensor = model(new_data_tensor)\n",
        "\n",
        "        all_predictions[f'Model_Fold_{i}'] = predictions_tensor.cpu().numpy().flatten()\n",
        "\n",
        "    # Create a final results DataFrame\n",
        "    results_df = pd.DataFrame(all_predictions, index=new_data_df.index)\n",
        "    results_df['Ensemble_Average_DTT'] = results_df.mean(axis=1)\n",
        "\n",
        "    print(\"\\n✅ Prediction Complete. Results:\\n\")\n",
        "    print(results_df.round(2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20ECsKRkTGys",
        "outputId": "b9ebdb2d-ae9a-4c40-c98c-c693ad968699"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "\n",
            "--- Evaluating Accuracy of Saved Models ---\n",
            "  - best_model_fold_1.pth: Accuracy = 0.8233\n",
            "  - best_model_fold_2.pth: Accuracy = 0.9459\n",
            "  - best_model_fold_3.pth: Accuracy = 0.9384\n",
            "  - best_model_fold_4.pth: Accuracy = 0.9417\n",
            "  - best_model_fold_5.pth: Accuracy = 0.9285\n",
            "\n",
            "Average Accuracy: 0.9156\n",
            "\n",
            "--- Predicting DTT for New Data from 'new_simulated_maize.csv' ---\n",
            "\n",
            "✅ Prediction Complete. Results:\n",
            "\n",
            "                Model_Fold_1  Model_Fold_2  Model_Fold_3  Model_Fold_4  \\\n",
            "IID                                                                      \n",
            "Maize_Sample_1     93.419998     92.849998     91.199997     89.730003   \n",
            "Maize_Sample_2     93.070000     93.879997     90.860001     87.940002   \n",
            "Maize_Sample_3     93.430000     93.760002     92.230003     89.620003   \n",
            "Maize_Sample_4     93.489998     93.779999     91.260002     88.610001   \n",
            "Maize_Sample_5     93.639999     93.430000     93.379997     90.250000   \n",
            "\n",
            "                Model_Fold_5  Ensemble_Average_DTT  \n",
            "IID                                                 \n",
            "Maize_Sample_1     90.949997             91.629997  \n",
            "Maize_Sample_2     88.959999             90.940002  \n",
            "Maize_Sample_3     91.599998             92.129997  \n",
            "Maize_Sample_4     90.059998             91.440002  \n",
            "Maize_Sample_5     90.989998             92.339996  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GS90rua6Whwx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}